{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ¥ Clinical Triage Router\n",
                "\n",
                "**A Specialized SLM Agent (â‰¤3B) for Patient Intake Classification**\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ“‹ Overview\n",
                "\n",
                "| Phase | Description |\n",
                "|-------|-------------|\n",
                "| **Phase 1** | Load pre-generated training data (or regenerate via API) |\n",
                "| **Phase 2** | Fine-tune Qwen2.5-1.5B with LoRA (4-bit quantized) |\n",
                "| **Phase 3** | Agentic loop with self-correction |\n",
                "| **Phase 4** | Evaluation with JSON validity & routing accuracy |\n",
                "\n",
                "### Key Features\n",
                "- âœ… **Zero-cost**: Runs entirely on Colab Free Tier (T4 GPU)\n",
                "- âœ… **Small Model**: Qwen2.5-1.5B (1.5B parameters, 4-bit)\n",
                "- âœ… **Self-Correction**: 3-retry loop with error context injection\n",
                "- âœ… **Strict Schema**: Pydantic validation for JSON tool calls\n",
                "\n",
                "---\n",
                "\n",
                "**Author**: Adarsh P  \n",
                "**Runtime**: Google Colab T4 GPU (Free Tier)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ”§ Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (optimized for Colab T4 GPU)\n",
                "!pip install -q unsloth\n",
                "!pip install -q --no-deps xformers trl peft accelerate bitsandbytes\n",
                "!pip install -q google-generativeai pydantic datasets nest_asyncio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone the repository\n",
                "!git clone https://github.com/forkadarshp/clinic-triage.git\n",
                "%cd clinic-triage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify imports and configuration\n",
                "from src import config\n",
                "from src.schemas import ToolName, parse_triage_output, get_mock_response\n",
                "\n",
                "print(\"ğŸ“¦ Configuration:\")\n",
                "print(f\"   Model: {config.MODEL_NAME}\")\n",
                "print(f\"   Tools: {[t.value for t in ToolName]}\")\n",
                "print(f\"   Training Examples: {config.NUM_TRAINING_EXAMPLES}\")\n",
                "print(\"\\nâœ… All imports successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“‹ Phase 1: Training Data\n",
                "\n",
                "> **Default Behavior**: Uses pre-generated training data (`data/train.jsonl`) for fast, reliable execution.  \n",
                "> **To regenerate**: Set `REGENERATE_DATA = True` below (requires API key in Colab Secrets)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import userdata\n",
                "\n",
                "# Load API keys from Colab Secrets (only needed if REGENERATE_DATA = True)\n",
                "try:\n",
                "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
                "    print(\"âœ… GOOGLE_API_KEY loaded from Colab Secrets\")\n",
                "except:\n",
                "    print(\"â„¹ï¸ GOOGLE_API_KEY not found (not needed for pre-generated data)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.data_generator import generate_training_data, load_training_data\n",
                "\n",
                "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
                "# â•‘  CONFIGURATION FLAG                                               â•‘\n",
                "# â•‘  Set to True to regenerate training data via Gemini API           â•‘\n",
                "# â•‘  Set to False (default) to use pre-generated data/train.jsonl     â•‘\n",
                "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "REGENERATE_DATA = False\n",
                "\n",
                "# Load or generate training examples\n",
                "examples = generate_training_data(\n",
                "    num_examples=config.NUM_TRAINING_EXAMPLES,\n",
                "    force_use_existing=not REGENERATE_DATA\n",
                ")\n",
                "\n",
                "print(f\"\\nğŸ“Š Loaded {len(examples)} training examples\")\n",
                "print(f\"\\nğŸ“ Sample Example:\")\n",
                "print(f\"   Query: {examples[0]['query'][:80]}...\")\n",
                "print(f\"   Tool: {examples[0]['tool']}\")\n",
                "print(f\"   Args: {examples[0]['arguments']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Load Model (4-bit Quantized)\n",
                "\n",
                "Using Unsloth's pre-quantized Qwen2.5-1.5B for memory efficiency on T4 GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.trainer import load_model_and_tokenizer, prepare_model_for_training\n",
                "\n",
                "# Load pre-quantized model (fits in ~6GB VRAM)\n",
                "model, tokenizer = load_model_and_tokenizer()\n",
                "print(f\"âœ… Loaded: {config.MODEL_NAME}\")\n",
                "print(f\"   Quantization: 4-bit\")\n",
                "print(f\"   Max Sequence Length: {config.MAX_SEQ_LENGTH}\")\n",
                "\n",
                "# Add LoRA adapters for efficient fine-tuning\n",
                "model = prepare_model_for_training(model)\n",
                "print(f\"\\nâœ… LoRA adapters attached\")\n",
                "print(f\"   Rank (r): {config.LORA_R}\")\n",
                "print(f\"   Alpha: {config.LORA_ALPHA}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 Fine-Tune\n",
                "\n",
                "Training the model to output structured JSON tool calls."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.trainer import prepare_dataset, train\n",
                "\n",
                "# Prepare dataset with instruction-output pairs\n",
                "dataset = prepare_dataset(tokenizer)\n",
                "print(f\"ğŸ“Š Training set: {len(dataset)} examples\")\n",
                "\n",
                "# Train with SFT (Supervised Fine-Tuning)\n",
                "model = train(model, tokenizer, dataset)\n",
                "print(\"\\nâœ… Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ¤– Phase 2: The Agentic Loop\n",
                "\n",
                "The agent includes:\n",
                "- **JSON Extraction**: Robust parsing with markdown code block handling\n",
                "- **Self-Correction**: 3-retry loop with error context injection\n",
                "- **Mock Execution**: Simulated system responses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "\n",
                "# Optimize model for inference (disables training-only layers)\n",
                "FastLanguageModel.for_inference(model)\n",
                "print(\"âœ… Model ready for inference\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.agent import TriageAgent\n",
                "\n",
                "# Initialize the triage agent\n",
                "agent = TriageAgent(model=model, tokenizer=tokenizer)\n",
                "print(\"âœ… TriageAgent initialized\")\n",
                "print(f\"   Max Retries: {config.MAX_RETRIES}\")\n",
                "print(f\"   Temperature: {config.GENERATION_TEMPERATURE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "#  DEMO: Emergency Case\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "test_query = \"\"\"65yo male, crushing chest pain radiating to jaw and left arm.\n",
                "Started 20 minutes ago. Diaphoretic, nauseous. History of hypertension.\n",
                "Location: 789 Pine Street.\"\"\"\n",
                "\n",
                "print(\"ğŸ“‹ INPUT Query:\")\n",
                "print(\"-\" * 60)\n",
                "print(test_query)\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# Run the agent\n",
                "output, response, metadata = agent.run(test_query)\n",
                "\n",
                "print(\"\\nğŸ“¤ OUTPUT:\")\n",
                "print(\"-\" * 60)\n",
                "print(f\"Tool Called: {output.tool if output else 'FAILED'}\")\n",
                "print(f\"Arguments:   {output.arguments.model_dump() if output else 'N/A'}\")\n",
                "print(f\"\\nğŸ’¬ Mock Response: {response}\")\n",
                "print(f\"\\nğŸ”„ Attempts: {metadata['attempts']} (max: {config.MAX_RETRIES})\")\n",
                "if metadata.get('errors'):\n",
                "    print(f\"âš ï¸  Errors encountered: {metadata['errors']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Convenience Function: `run_triage_agent(query)`\n",
                "\n",
                "For easier integration, use the standalone function:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.agent import run_triage_agent\n",
                "\n",
                "# Quick inference with pre-loaded model\n",
                "result = run_triage_agent(\n",
                "    query=\"45yo female with severe abdominal pain, nausea. Location: 123 Main St.\",\n",
                "    model=model,\n",
                "    tokenizer=tokenizer\n",
                ")\n",
                "\n",
                "print(\"Result:\", result[\"output\"])\n",
                "print(\"Mock Response:\", result[\"response\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“Š Phase 3: Evaluation\n",
                "\n",
                "Testing against a held-out dataset with metrics:\n",
                "- **JSON Validity**: % of outputs that parse correctly\n",
                "- **Routing Accuracy**: % of correct tool selections"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.evaluator import load_test_data, evaluate, print_report\n",
                "\n",
                "# Load held-out test cases\n",
                "test_data = load_test_data()\n",
                "print(f\"ğŸ“‹ Loaded {len(test_data)} test cases\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run evaluation (this may take a few minutes)\n",
                "results = evaluate(agent, test_data, verbose=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print final evaluation report\n",
                "print_report(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ğŸ“ Summary\n",
                "\n",
                "### Results\n",
                "\n",
                "| Metric | Result |\n",
                "|--------|--------|\n",
                "| **Model** | Qwen2.5-1.5B (4-bit quantized) |\n",
                "| **Training Examples** | 100 |\n",
                "| **JSON Validity** | See report above |\n",
                "| **Routing Accuracy** | See report above |\n",
                "\n",
                "### Task Requirements Checklist\n",
                "\n",
                "| Requirement | Status |\n",
                "|-------------|--------|\n",
                "| Runs on Colab Free Tier (T4 GPU) | âœ… |\n",
                "| Model size â‰¤ 3B parameters | âœ… (1.5B) |\n",
                "| Outputs valid JSON | âœ… |\n",
                "| Handles 3 tools exactly | âœ… |\n",
                "| Self-correction/error handling | âœ… |\n",
                "| Evaluation on 10+ test cases | âœ… |\n",
                "| Zero-cost execution | âœ… |\n",
                "\n",
                "### Key Implementation Highlights\n",
                "\n",
                "1. **Efficient Fine-Tuning**: LoRA adapters with 4-bit quantization fit in T4 VRAM\n",
                "2. **Robust Parsing**: Handles markdown code blocks, partial JSON, and edge cases\n",
                "3. **Self-Correction**: Error context injection enables recovery from failures\n",
                "4. **Strict Validation**: Pydantic schemas ensure exact JSON structure"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.14.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
