{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfe5 Clinical Triage Router\n",
    "\n",
    "**A specialized SLM agent (\u22643B) for patient intake classification**\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Synthetic Data Generation** - Gemini 1.5 Flash (free tier)\n",
    "2. **Fine-Tuning** - Unsloth + LoRA on Qwen2.5-1.5B (4-bit)\n",
    "3. **Agentic Loop** - Self-correction with retry logic\n",
    "4. **Evaluation** - JSON validity & routing accuracy metrics\n",
    "\n",
    "---\n",
    "**Author**: Adarsh P  \n",
    "**Runtime**: Google Colab T4 GPU (Free Tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab T4 GPU)\n",
    "!pip install -q unsloth\n",
    "!pip install -q --no-deps xformers trl peft accelerate bitsandbytes\n",
    "!pip install -q google-generativeai pydantic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/forkadarshp/clinic-triage.git\n",
    "%cd clinic-triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imports\n",
    "from src import config\n",
    "from src.schemas import ToolName, parse_triage_output, get_mock_response\n",
    "\n",
    "print(f\"\u2705 Model: {config.MODEL_NAME}\")\n",
    "print(f\"\u2705 Tools: {[t.value for t in ToolName]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udccb Phase 1: Synthetic Data Generation & Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Training Data (Gemini 1.5 Flash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set API keys (Gemini or OpenAI supported)\n",
    "try:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
    "except: pass\n",
    "\n",
    "try:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "except: pass\n",
    "\n",
    "print(\"\u2705 API keys configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_generator import generate_training_data, load_training_data\n",
    "\n",
    "# Generate 100 training examples\n",
    "examples = generate_training_data(num_examples=100)\n",
    "print(f\"\\n\ud83d\udcca Generated {len(examples)} examples\")\n",
    "print(f\"\\nSample:\\n{examples[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Model (4-bit Quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import load_model_and_tokenizer, prepare_model_for_training\n",
    "\n",
    "# Load pre-quantized model\n",
    "model, tokenizer = load_model_and_tokenizer()\n",
    "print(f\"\u2705 Loaded {config.MODEL_NAME}\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = prepare_model_for_training(model)\n",
    "print(f\"\u2705 LoRA adapters added (r={config.LORA_R})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import prepare_dataset, train\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = prepare_dataset(tokenizer)\n",
    "print(f\"\ud83d\udcca Training set: {len(dataset)} examples\")\n",
    "\n",
    "# Train\n",
    "model = train(model, tokenizer, dataset)\n",
    "print(\"\\n\u2705 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83e\udd16 Phase 2: The Agentic Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent import TriageAgent\n",
    "\n",
    "# Initialize agent with trained model\n",
    "agent = TriageAgent(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "test_query = \"\"\"65yo male, crushing chest pain radiating to jaw and left arm.\n",
    "Started 20 minutes ago. Diaphoretic, nauseous. History of hypertension.\n",
    "Location: 789 Pine Street.\"\"\"\n",
    "\n",
    "output, response, metadata = agent.run(test_query)\n",
    "\n",
    "print(\"\ud83d\udccb Query:\", test_query[:80], \"...\")\n",
    "print(\"\\n\ud83d\udd27 Tool Called:\", output.tool if output else \"FAILED\")\n",
    "print(\"\ud83d\udce6 Arguments:\", output.arguments.model_dump() if output else \"N/A\")\n",
    "print(\"\\n\ud83d\udcac Response:\", response)\n",
    "print(f\"\\n\ud83d\udd04 Attempts: {metadata['attempts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcca Phase 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator import load_test_data, evaluate, print_report\n",
    "\n",
    "# Load 10 held-out test cases\n",
    "test_data = load_test_data()\n",
    "print(f\"\ud83d\udccb Loaded {len(test_data)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results = evaluate(agent, test_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final report\n",
    "print_report(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcdd Summary\n",
    "\n",
    "| Metric | Result |\n",
    "|--------|--------|\n",
    "| **Model** | Qwen2.5-1.5B (4-bit) |\n",
    "| **Training Examples** | 100 |\n",
    "| **JSON Validity** | TBD% |\n",
    "| **Routing Accuracy** | TBD% |\n",
    "\n",
    "### Key Features\n",
    "- \u2705 Runs on Colab Free Tier (T4 GPU)\n",
    "- \u2705 4-bit quantization for memory efficiency\n",
    "- \u2705 Self-correction with 3-retry logic\n",
    "- \u2705 Pydantic validation for strict schema adherence\n",
    "- \u2705 Gemini Flash for zero-cost data generation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
